# Daily Assessment Checklists

## Overview
These checklists provide instructors with standardized evaluation criteria for consistent assessment across all daily exercises. Each checklist corresponds to specific learning objectives and competency requirements.

---

## Week 1 Foundation Checklists

### Day 1: AI Tools Deep Dive Checklist
**Exercise Reference:** [Day 1 AI Tools](../weekly/week1/day01-ai-tools-deep-dive.md)

#### Pre-Assessment Setup
- [ ] Verify student has access to required AI tools (ChatGPT, Claude, Midjourney, etc.)
- [ ] Confirm understanding of assessment criteria and rubric
- [ ] Set up screen recording/documentation requirements
- [ ] Establish time limits and submission format

#### AI Tool Proficiency Assessment (50 points)
**Excellent (45-50 pts)**
- [ ] Uses 5+ different AI tools effectively
- [ ] Demonstrates advanced features and capabilities
- [ ] Shows understanding of tool limitations and best practices
- [ ] Creates sophisticated, multi-step workflows
- [ ] Exhibits creative and innovative applications

**Proficient (35-44 pts)**
- [ ] Uses 3-4 AI tools competently
- [ ] Demonstrates basic to intermediate features
- [ ] Shows good understanding of capabilities
- [ ] Creates functional workflows
- [ ] Shows practical applications

**Developing (25-34 pts)**
- [ ] Uses 2-3 AI tools with basic competency
- [ ] Limited feature demonstration
- [ ] Basic understanding of capabilities
- [ ] Simple workflows
- [ ] Limited applications

**Inadequate (0-24 pts)**
- [ ] Uses fewer than 2 tools or shows poor competency
- [ ] No clear understanding of capabilities
- [ ] Cannot create functional workflows
- [ ] No practical applications demonstrated

#### Workflow Integration Assessment (30 points)
**Excellent (27-30 pts)**
- [ ] Seamless integration across multiple tools
- [ ] Sophisticated automation and efficiency optimization
- [ ] Clear documentation of workflow steps
- [ ] Measurable productivity improvements
- [ ] Professional presentation of results

**Common Issues to Watch For:**
- [ ] Tool switching without clear purpose
- [ ] Over-reliance on single tool
- [ ] Inefficient prompt engineering
- [ ] Poor documentation of process
- [ ] No evidence of optimization thinking

#### Problem-Solving Application Assessment (20 points)
**Evaluation Criteria:**
- [ ] Clear problem identification
- [ ] Creative use of AI tools for solution
- [ ] Evidence-based approach
- [ ] Measurable results or outcomes
- [ ] Professional documentation

**Red Flags:**
- [ ] Random tool usage without strategy
- [ ] No clear problem being solved
- [ ] Over-dependence on AI without human judgment
- [ ] Unethical or inappropriate AI usage

### Day 2: Customer Interview Mastery Checklist
**Exercise Reference:** [Day 2 Customer Interviews](../weekly/week1/day02-customer-interview-mastery.md)

#### Interview Methodology Assessment (40 points)
**Pre-Interview Evaluation:**
- [ ] Prepared interview guide with open-ended questions
- [ ] Identified target customer personas
- [ ] Set up recording/note-taking system
- [ ] Established interview objectives

**During Interview Evaluation:**
- [ ] Uses open-ended questions effectively
- [ ] Avoids leading or biased questions
- [ ] Demonstrates active listening skills
- [ ] Follows up on interesting responses
- [ ] Maintains professional demeanor

**Post-Interview Evaluation:**
- [ ] Systematic documentation of insights
- [ ] Identification of patterns and themes
- [ ] Clear separation of facts vs. opinions
- [ ] Actionable recommendations

#### Data Quality & Analysis Assessment (35 points)
**Data Collection Quality:**
- [ ] Rich, detailed responses captured
- [ ] Multiple perspectives gathered
- [ ] Demographic diversity in sample
- [ ] Sufficient depth of information

**Analysis Sophistication:**
- [ ] Systematic analysis methodology
- [ ] Pattern recognition across interviews
- [ ] Insight extraction beyond surface level
- [ ] Statistical thinking where appropriate

#### Research Application Assessment (25 points)
**Business Impact:**
- [ ] Clear connection to business decisions
- [ ] Hypothesis validation/invalidation
- [ ] Strategic implications identified
- [ ] Next steps clearly defined

### Day 3: Market Sizing Mastery Checklist
**Exercise Reference:** [Day 3 Market Sizing](../weekly/week1/day03-market-sizing-mastery.md)

#### Market Analysis Methodology Assessment (40 points)
**Approach Evaluation:**
- [ ] Uses multiple sizing methodologies (TAM/SAM/SOM)
- [ ] Bottom-up and top-down analysis
- [ ] Clear assumptions documentation
- [ ] Logical calculation steps

**Sophistication Indicators:**
- [ ] Considers market dynamics and trends
- [ ] Accounts for competitive landscape
- [ ] Includes growth projections
- [ ] Risk and sensitivity analysis

#### Data Sources & Validation Assessment (35 points)
**Source Quality:**
- [ ] Uses credible, authoritative sources
- [ ] Multiple data source triangulation
- [ ] Recent and relevant data
- [ ] Primary research integration

**Validation Process:**
- [ ] Cross-reference verification
- [ ] Sanity check calculations
- [ ] Expert opinion integration
- [ ] Conservative vs. optimistic scenarios

#### Strategic Implications Assessment (25 points)
**Strategic Thinking:**
- [ ] Clear market opportunity sizing
- [ ] Competitive positioning insights
- [ ] Go-to-market implications
- [ ] Investment/resource requirements

---

## Week 2 Development Checklists

### Day 8: Platform Mastery Checklist
**Exercise Reference:** [Day 8 Platform Mastery](../weekly/week2/day8-platform-mastery.md)

#### Technical Implementation Assessment (50 points)
**Platform Utilization:**
- [ ] Uses advanced platform features effectively
- [ ] Demonstrates sophisticated architecture understanding
- [ ] Implements efficient workflows
- [ ] Shows optimization thinking

**Quality Indicators:**
- [ ] Clean, organized project structure
- [ ] Proper use of platform best practices
- [ ] Evidence of testing and debugging
- [ ] Professional documentation

#### Scalability Design Assessment (30 points)
**Future-Proofing:**
- [ ] Considers growth and scaling scenarios
- [ ] Implements modular architecture
- [ ] Plans for performance optimization
- [ ] Documents scaling considerations

#### Development Efficiency Assessment (20 points)
**Productivity Indicators:**
- [ ] Rapid development cycles
- [ ] Efficient debugging processes
- [ ] Good time management
- [ ] Clear development documentation

---

## Assessment Best Practices

### Instructor Guidelines

#### Consistency Standards
- [ ] Use rubrics consistently across all students
- [ ] Document rationale for scores in borderline cases
- [ ] Calibrate with other instructors regularly
- [ ] Provide specific, actionable feedback

#### Time Management
- [ ] Allocate specific time slots for each assessment component
- [ ] Use timer to maintain consistency
- [ ] Allow for brief clarification questions
- [ ] Document any assessment accommodations

#### Feedback Quality
- [ ] Provide specific examples of excellence/improvement areas
- [ ] Connect feedback to learning objectives
- [ ] Offer concrete next steps for improvement
- [ ] Balance constructive criticism with encouragement

### Common Assessment Pitfalls

#### Instructor Pitfalls
- [ ] ⚠️ Inconsistent application of rubrics
- [ ] ⚠️ Letting personal preferences influence scoring
- [ ] ⚠️ Rushing through assessments
- [ ] ⚠️ Not providing sufficient feedback detail

#### Student Issues to Monitor
- [ ] ⚠️ Superficial completion without deep understanding
- [ ] ⚠️ Over-reliance on AI without human insight
- [ ] ⚠️ Poor time management leading to rushed work
- [ ] ⚠️ Not following assessment guidelines

### Documentation Requirements

#### For Each Assessment
- [ ] Student name and submission timestamp
- [ ] Detailed scores for each rubric component
- [ ] Specific feedback comments
- [ ] Areas of excellence identified
- [ ] Improvement recommendations
- [ ] Pass/fail decision with rationale

#### Weekly Summary Reports
- [ ] Class performance trends
- [ ] Common challenges identified
- [ ] Exceptional performances highlighted
- [ ] Curriculum adjustment recommendations

---

## Emergency Procedures

### Student Performance Issues
1. **Immediate Support Needed (Score <50%)**
   - [ ] Schedule one-on-one meeting within 24 hours
   - [ ] Assess foundational skill gaps
   - [ ] Create remediation plan
   - [ ] Consider additional resources/tutoring

2. **Improvement Needed (Score 50-69%)**
   - [ ] Provide detailed written feedback
   - [ ] Suggest specific improvement strategies
   - [ ] Monitor next assessment closely
   - [ ] Offer optional office hours

3. **Borderline Performance (Score 70-74%)**
   - [ ] Encourage continued effort
   - [ ] Identify specific growth opportunities
   - [ ] Provide additional practice resources

### Technical Issues
- [ ] Alternative assessment methods for platform failures
- [ ] Backup submission procedures
- [ ] Extended deadlines for technical difficulties
- [ ] Alternative demonstration formats

---

*These checklists ensure consistent, fair, and comprehensive assessment of student progress while maintaining high standards and supporting student success throughout the AI-powered MVP development course.*